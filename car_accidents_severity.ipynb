{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "In this challenge, you'll try to predict the severity of car accidents, based on features collected from after-crash police investigation\n",
    "\n",
    "This [Kaggle challenge](https://www.kaggle.com/c/accident-severity) comprises of 1,000,000 accidents report, split into multiple `.csv` files.\n",
    "\n",
    "**The goal of the model is to predict the severity of car accidents**. The target variable is called `grav` (for 'gravity') in the file `users.csv`. This variable has four levels, but in this challenge, we'll convert it to a binary classification problem. We will:\n",
    "- Load data into pandas\n",
    "- Create a single DataFrame for our problem, where each row is a user involved in an accident\n",
    "- Extract the features you think would be relevant to predict its severity\n",
    "- Build a data pipeline that gives you a baseline model\n",
    "- Then, iterate on the different phase and try to get the best model! \n",
    "\n",
    "üî• **Today is a special challenge** :\n",
    "- You will send your best score to your batch slack channel!\n",
    "- The winner will present its notebook to the class during the recap session at 5pm üí™"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "---\n",
    "‚ö†Ô∏è **Good practices to follow for large exploratory notebooks**\n",
    "- Build your Notebook linearily so that it can always be run from top to bottom without any errors\n",
    "- Clean the outputs of your cells that are not needed\n",
    "- Clean your variables in memory when you don't need them (especially when they are very large). You can use the python built-in function `del`, or the the **Jupyter nbextentions** `variable_inspector`\n",
    "- Make heavy use of `table_of_content` and `collapsable_headings` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data sourcing\n",
    "\n",
    "Let's get started! The data we want to use is from the `csv` files in `/data/data_training`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jessicaparker/.pyenv/versions/3.7.7/envs/lewagon/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3072: DtypeWarning: Columns (15) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "cara = pd.read_csv(\"data_training/caracteristics.csv\", encoding=\"ISO-8859-1\")\n",
    "users = pd.read_csv(\"data_training/users.csv\", encoding=\"ISO-8859-1\")\n",
    "places = pd.read_csv(\"data_training/places.csv\", encoding=\"ISO-8859-1\")\n",
    "vehicles = pd.read_csv(\"data_training/vehicles.csv\", encoding=\"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì Explore the different tables, and the different variables using `challenge_variable.md`, which provides a description of features. More details can be found [here](https://static.data.gouv.fr/resources/base-de-donnees-accidents-corporels-de-la-circulation/20180927-112352/description-des-bases-de-donnees-onisr-annees-2005-a-2017.pdf) if needed, or in the [Kaggle](https://www.kaggle.com/ahmedlahlou/accidents-in-france-from-2005-to-2016/discussion) discussion channel. Understand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì We will create one single dataset where each row should represent a `user` in a car, by merging the data from the different files dataset.  \n",
    "**Take some time to think about how you would do it yourself**, and only then, read carefully through the code below to understand exactly what we did"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge caracteristics and places on 'Num_Acc'\n",
    "data = cara.merge(places, on='Num_Acc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a common key to merge users amd vehicles on\n",
    "users['Num_Acc_num_veh'] = users['Num_Acc'].map(lambda x: str(x)) + users['num_veh']\n",
    "vehicles['Num_Acc_num_veh'] = vehicles['Num_Acc'].map(lambda x: str(x)) + vehicles['num_veh']\n",
    "# Remove useless columns\n",
    "vehicles = vehicles.drop(columns=['index'])\n",
    "users = users.drop(columns=['index', 'Num_Acc', 'num_veh'])\n",
    "# Merge vehicles and users\n",
    "tmp = vehicles.merge(users, on='Num_Acc_num_veh', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge all datasets on 'Num_Acc'\n",
    "data = data.merge(tmp, on='Num_Acc', how='inner')\n",
    "del tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    492921\n",
       "4    430413\n",
       "3    253787\n",
       "2     32241\n",
       "Name: grav, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"grav\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "\n",
    "We will apply some preprocessing methods like standardization or missing values removal or imputing.\n",
    "Remember to look at `challenge_variable.md` for a description of features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop lines without targets (if any)\n",
    "data_cleaned = data[~np.isnan(data.grav)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "v2                 0.953958\n",
       "lat                0.632648\n",
       "long               0.632648\n",
       "gps                0.631134\n",
       "pr1                0.463472\n",
       "pr                 0.462609\n",
       "v1                 0.383683\n",
       "adr                0.184124\n",
       "voie               0.048149\n",
       "place              0.032898\n",
       "secu               0.011189\n",
       "lartpc             0.003175\n",
       "larrout            0.002416\n",
       "an_nais            0.001841\n",
       "nbv                0.001731\n",
       "vosp               0.001060\n",
       "actp               0.000882\n",
       "locp               0.000867\n",
       "etatp              0.000825\n",
       "infra              0.000662\n",
       "env1               0.000653\n",
       "plan               0.000445\n",
       "prof               0.000443\n",
       "surf               0.000427\n",
       "obs                0.000427\n",
       "circ               0.000392\n",
       "situ               0.000355\n",
       "obsm               0.000236\n",
       "trajet             0.000186\n",
       "manv               0.000106\n",
       "choc               0.000064\n",
       "atm                0.000039\n",
       "col                0.000017\n",
       "com                0.000005\n",
       "catr               0.000002\n",
       "hrmn               0.000000\n",
       "lum                0.000000\n",
       "agg                0.000000\n",
       "int                0.000000\n",
       "jour               0.000000\n",
       "mois               0.000000\n",
       "an                 0.000000\n",
       "Num_Acc            0.000000\n",
       "occutc             0.000000\n",
       "sexe               0.000000\n",
       "dep                0.000000\n",
       "index_y            0.000000\n",
       "catv               0.000000\n",
       "grav               0.000000\n",
       "catu               0.000000\n",
       "Num_Acc_num_veh    0.000000\n",
       "num_veh            0.000000\n",
       "senc               0.000000\n",
       "index_x            0.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check whih features with highest ratio of NaN per column\n",
    "(data_cleaned.isna().sum() / data_cleaned.shape[0]).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove too incomplete features\n",
    "too_incomplete_features=[\n",
    "    'locp', 'actp', 'etatp'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove features that can be safely considered useless for the predictive power of our model\n",
    "useless_features=[\n",
    "    'v2', 'lat', 'long', 'gps', 'pr1', 'pr', 'v1', 'adr', 'voie',\n",
    "    'index_x', 'Num_Acc', 'Num_Acc_num_veh', 'Num_Acc', 'num_veh', 'index_y',\n",
    "    'jour', 'an',\n",
    "    'dep', 'com', 'env1',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cleaned.drop(columns=too_incomplete_features+useless_features, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['mois', 'hrmn', 'lum', 'agg', 'int', 'atm', 'col', 'catr', 'circ',\n",
       "       'nbv', 'vosp', 'prof', 'plan', 'lartpc', 'larrout', 'surf', 'infra',\n",
       "       'situ', 'senc', 'catv', 'occutc', 'obs', 'obsm', 'choc', 'manv',\n",
       "       'place', 'catu', 'grav', 'sexe', 'trajet', 'secu', 'an_nais'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_cleaned.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0    1198624\n",
       "1.0       8471\n",
       "2.0       2267\n",
       "Name: senc, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_cleaned[\"senc\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a `data_cleaned` dataset! Let's now engineer our features as needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare features and target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numerical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mois</th>\n",
       "      <th>hrmn</th>\n",
       "      <th>lum</th>\n",
       "      <th>agg</th>\n",
       "      <th>int</th>\n",
       "      <th>atm</th>\n",
       "      <th>col</th>\n",
       "      <th>catr</th>\n",
       "      <th>circ</th>\n",
       "      <th>nbv</th>\n",
       "      <th>...</th>\n",
       "      <th>obsm</th>\n",
       "      <th>choc</th>\n",
       "      <th>manv</th>\n",
       "      <th>place</th>\n",
       "      <th>catu</th>\n",
       "      <th>grav</th>\n",
       "      <th>sexe</th>\n",
       "      <th>trajet</th>\n",
       "      <th>secu</th>\n",
       "      <th>an_nais</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11</td>\n",
       "      <td>130</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>1994.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11</td>\n",
       "      <td>130</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1967.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1145</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1956.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1145</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1948.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1145</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1950.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows √ó 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   mois  hrmn  lum  agg  int  atm  col  catr  circ  nbv  ...  obsm  choc  \\\n",
       "0    11   130    3    2    1  1.0  3.0   9.0   2.0  2.0  ...   0.0   1.0   \n",
       "1    11   130    3    2    1  1.0  3.0   9.0   2.0  2.0  ...   0.0   2.0   \n",
       "2     1  1145    1    2    2  7.0  3.0   3.0   2.0  2.0  ...   2.0   2.0   \n",
       "3     1  1145    1    2    2  7.0  3.0   3.0   2.0  2.0  ...   2.0   8.0   \n",
       "4     1  1145    1    2    2  7.0  3.0   3.0   2.0  2.0  ...   2.0   8.0   \n",
       "\n",
       "   manv  place  catu  grav  sexe  trajet  secu  an_nais  \n",
       "0   1.0    1.0     1     3     1     5.0  21.0   1994.0  \n",
       "1  16.0    1.0     1     1     1     1.0  11.0   1967.0  \n",
       "2  19.0    1.0     1     1     1     5.0  11.0   1956.0  \n",
       "3   1.0    1.0     1     1     1     5.0  11.0   1948.0  \n",
       "4   1.0    3.0     2     3     2     0.0  11.0   1950.0  \n",
       "\n",
       "[5 rows x 32 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_cleaned.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List numerical features to process\n",
    "features_numerical = [\"nbv\",\"lartpc\",\"larrout\",\"occutc\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jessicaparker/.pyenv/versions/3.7.7/envs/lewagon/lib/python3.7/site-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return array(a, dtype, copy=False, order=order)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1159bbf10>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "data_cleaned[features_numerical].boxplot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import RobustScaler, StandardScaler\n",
    "\n",
    "imp_mean = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "data_cleaned[features_numerical] = imp_mean.fit_transform(data_cleaned[features_numerical])\n",
    "\n",
    "def preprocess_numerical_features(X):\n",
    "#     Returns a new DataFrame with\n",
    "#     - Missing values replaced by Column Mean\n",
    "#     - Features Standard Scaled\n",
    "#     - Original Features names kept in the DataFrame\n",
    "        ct = ColumnTransformer([\n",
    "         (\"s_scaler\", StandardScaler(), ['nbv']),\n",
    "         (\"r_scaler\", RobustScaler(), ['lartpc', 'larrout','occutc'])])\n",
    "        X = ct.fit_transform(X)\n",
    "        return X\n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cleaned[features_numerical] = preprocess_numerical_features(data_cleaned[features_numerical])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nbv        0\n",
       "lartpc     0\n",
       "larrout    0\n",
       "occutc     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check your code below\n",
    "data_cleaned[features_numerical].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nbv</th>\n",
       "      <th>lartpc</th>\n",
       "      <th>larrout</th>\n",
       "      <th>occutc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.052801</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.341176</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.052801</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.341176</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.052801</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.070588</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.052801</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.070588</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.052801</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.070588</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1209357</th>\n",
       "      <td>-1.248930</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.047059</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1209358</th>\n",
       "      <td>-1.248930</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.047059</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1209359</th>\n",
       "      <td>-1.248930</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.047059</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1209360</th>\n",
       "      <td>-0.052801</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.164706</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1209361</th>\n",
       "      <td>-0.052801</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.164706</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1209362 rows √ó 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              nbv  lartpc   larrout  occutc\n",
       "0       -0.052801     0.0 -0.341176     0.0\n",
       "1       -0.052801     0.0 -0.341176     0.0\n",
       "2       -0.052801     0.0 -0.070588     0.0\n",
       "3       -0.052801     0.0 -0.070588     0.0\n",
       "4       -0.052801     0.0 -0.070588     0.0\n",
       "...           ...     ...       ...     ...\n",
       "1209357 -1.248930     0.0  1.047059     0.0\n",
       "1209358 -1.248930     0.0  1.047059     0.0\n",
       "1209359 -1.248930     0.0  1.047059     0.0\n",
       "1209360 -0.052801    20.0  0.164706     0.0\n",
       "1209361 -0.052801    20.0  0.164706     0.0\n",
       "\n",
       "[1209362 rows x 4 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check your code below\n",
    "data_cleaned[features_numerical]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì Do you get a Warning \"A value is trying to be set on a copy of a slice from a DataFrame\"?\n",
    "If so, it may be because you are trying to modify the input DataFrame `data_cleaned`!\n",
    "\n",
    "Read this [important blog on copy vs. view](https://www.practicaldatascience.org/html/views_and_copies_in_pandas.html) of pandas DataFrame and try to solve your warning by yourself\n",
    "\n",
    "\n",
    "\n",
    "<details>\n",
    "    <summary>Hint</summary>\n",
    "\n",
    "`pd.DataFrame.copy()`\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (optional) cyclical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do you see any cyclical features to process specifically?\n",
    "# This can be done after a first baseline model is created.\n",
    "features_cyclical = ['hr_sin','hr_cos',\"mnth_sin\",\"mnth_cos\",\"an_nais\"]\n",
    "\n",
    "data_cleaned[\"hours\"] = data_cleaned[\"hrmn\"]//100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE BELOW\n",
    "\n",
    "data_cleaned['hr_sin'] = np.cos(data_cleaned[\"hours\"]*(2.*np.pi/24))\n",
    "data_cleaned['hr_cos'] = np.cos(data_cleaned[\"hours\"]*(2.*np.pi/24))\n",
    "data_cleaned['mnth_sin'] = np.sin((data_cleaned.mois-1)*(2.*np.pi/12))\n",
    "data_cleaned['mnth_cos'] = np.cos((data_cleaned.mois-1)*(2.*np.pi/12)) \n",
    "\n",
    "data_cleaned.drop('mois', axis=1, inplace = True)\n",
    "data_cleaned.drop('hours', axis=1,inplace = True)\n",
    "data_cleaned.drop('hrmn', axis=1, inplace = True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_cleaned.drop('mois', axis=1, inplace = True)\n",
    "# data_cleaned.drop('hours', axis=1,inplace = True)\n",
    "# data_cleaned.drop('hrmn', axis=1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Categorical features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì Create the last group of feature (categorical features) without hardcoding them manually. Then, create the associated preprocessing method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['int',\n",
       " 'atm',\n",
       " 'vosp',\n",
       " 'sexe',\n",
       " 'catu',\n",
       " 'manv',\n",
       " 'agg',\n",
       " 'obs',\n",
       " 'infra',\n",
       " 'trajet',\n",
       " 'obsm',\n",
       " 'place',\n",
       " 'col',\n",
       " 'secu',\n",
       " 'circ',\n",
       " 'surf',\n",
       " 'situ',\n",
       " 'lum',\n",
       " 'prof',\n",
       " 'senc',\n",
       " 'plan',\n",
       " 'catr',\n",
       " 'catv',\n",
       " 'choc']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_categorical = list(set(data_cleaned.columns) - set(features_numerical) - set(features_cyclical) - {'grav'})\n",
    "features_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1209362, 34)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_cleaned.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_categorical_features(X):   \n",
    "    features_categorical = list(set(X.columns) - set(features_numerical)  - {'grav'})\n",
    "    ''' Returns a new DataFrame with dummified columns'''\n",
    "    X = pd.get_dummies(X, columns = features_categorical)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì Create the new `data_preprocessed` dataset by concatenating all three preprocessing, and then drop all remaining NaN that could not have been handled previously despite our preprocessing. You should have a (1125397, 216) shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1209362, 24)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# YOUR CODE BELOW\n",
    "data_cleaned[features_categorical].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cleaned = pd.get_dummies(data_cleaned, columns = features_categorical)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cleaned.dropna(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1207135, 235)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_cleaned.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Dataset\n",
    "‚ùì Create X and y, and don't forget to convert the classification into a binary task.\n",
    "\n",
    "For instance:\n",
    "\n",
    "```python\n",
    "data['grav_binary'] = data['grav'].replace({1: 0, 4: 0, 2: 1, 3: 1})\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You should scale/fit/transform AFTER you split but for time's sake and the for sake of my sanity, I will follow the excercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cleaned_sample = data_cleaned.sample(1_000)\n",
    "data_cleaned_sample = data_cleaned.drop(\"an_nais\", axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create X and y\n",
    "X = data_cleaned_sample.drop('grav', axis=1)\n",
    "data_cleaned_sample ['grav_binary'] = data_cleaned_sample ['grav'].replace({1: 0, 4: 0, 2: 1, 3: 1})\n",
    "y = data_cleaned_sample ['grav_binary']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a smaller dataset (X_small, y_small) for investigation purpose only\n",
    "# X_small = X.sample(n = 10_000)\n",
    "# y_small = y.sample(n = 10_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Test Split both datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ##Because they had us scale before we split, I am applying the scaling methods here on the train/test sets. \n",
    "\n",
    "# imp_mean = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "# X_train[features_numerical] = imp_mean.fit_transform(X_train[features_numerical] )\n",
    "# X_train[features_numerical] = imp_mean.transform(X_train[features_numerical] )\n",
    "\n",
    "\n",
    "\n",
    "# X_train[features_numerical] = preprocess_numerical_features(X_train[features_numerical])\n",
    "# ##This is bot fitting and transforming the test data on the test data but oh well, moving on because of time. \n",
    "# X_test[features_numerical] = preprocess_numerical_features(X_test[features_numerical])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Features exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You now have a dataset ready for training! \n",
    "**Skip directly to section 5 to get a baseline model working ASAP**, and only then come back to this section 4 if you want to better understand your X and get inspiration for the best model to use, or for some feature selection to reduce model complexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùìInvestigate your X. Are features strongly correlated? Are some feature more important than other?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# import seaborn as sns\n",
    "# sns.heatmap(X.corr())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùìFit a PCA and plot the cumulated sum of explained variance ratio of your Principal Component. Do you see any clear elbow?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forest-based most important features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì Fit a default RandomForestClassifier on a small smaple to estimate the top 20 feature importance. Do they make intuitive sense to your point of view?  Do you see any clear elbow for dimension-reduction?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint\n",
    "\n",
    "forest = RandomForestClassifier()\n",
    "\n",
    "# Hyperparameter Grid\n",
    "search_space = {'min_samples_split' : randint(2,25), 'max_depth':randint(2,25)}\n",
    "\n",
    "# Instanciate Grid Search\n",
    "search = RandomizedSearchCV(forest, param_distributions = search_space, n_jobs=-1, scoring = 'accuracy', cv = 10, n_iter = 10)\n",
    "\n",
    "# Fit search\n",
    "search.fit(X_train,y_train)\n",
    "search.score(X_test, y_test)\n",
    "#Best parameters\n",
    "search.best_params_\n",
    "model = search.best_estimator_\n",
    "model.fit(X_train,y_train)\n",
    "model.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì (Optional) There are better ways to estimate feature importance in a RandomForest. Feel free to try to two following options\n",
    "\n",
    "**Option 1** : Recursive-method\n",
    "1. Train a first model, note top1 feature (computed based on the gini-explicative power of the feature, in each tree)\n",
    "2. Remove top1 from your X and retrain a RandomForest. Note top1 feature and it's relative importance\n",
    "3. Loop\n",
    "\n",
    "**Option 2** : Permutation-method ([sklearn.inspection.permutation_importance](https://scikit-learn.org/stable/modules/permutation_importance.html#permutation-importance)), works with any model!\n",
    "1. Train a first model, keep track of its accuracy\n",
    "2. Take one feature and shuffle its columns. Compute new accuracy of the corrupted dataset, and note by how much it has been reduced.\n",
    "3. Loop over all features and rank them by accuracy reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline performance metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì What is the class balance of your target?  \n",
    "\n",
    "What would be the most dumb baseline to beat? Print the `classification_report` of this dumb model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# instantiate model\n",
    "logreg = LogisticRegression()\n",
    "y_pred = logreg.fit(X_train, y_train).predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì If you don't want to favor any class over the other, what would be a good performance metric for your problem? \n",
    "Take some time to think before reading the answer! It's not that obvious."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Answer</summary>\n",
    "\n",
    "In such an unbalanced problem, accuracy is meaningless: A very dumb model predicting always zeros would have great accuracy, to the detriment of the predictive power of class  1, which has precision and recall equal to zero!\n",
    "    \n",
    "The non-weighted mean between both f1 score of each class called `f1_macro` would be a good measure for this type of problem.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Model (A first iteration)\n",
    "\n",
    "‚ùì Create a simple model, fast to train, to classify the severity of the accidents. Start simple. Don't forget to fit on your training set and evaluate the score on your test set. Can you beat the Baseline? What about its Accuracy? Measure the time it takes on the full dataset, with `%%time` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# YOUR CODE BELOW\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "#%%time\n",
    "log_model = LogisticRegression()\n",
    "log_model.fit(X_train,y_train)\n",
    "log_model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This does not really beat out \"dumb\" model...we can do a much better job! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üî•üî•üî• Advanced Models - LeWagon batch contest ! üî•üî•üî•"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì Now it's your turn to shine! Play with different models and try to find the best one on your training set!\n",
    "- Send your best score (as defined above) to your slack channel without saying which model you used!\n",
    "- ‚ö†Ô∏è Only send score tested on the `y_test` of complete size (1M+ rows!)\n",
    "- Feel free to use your X_small for investigation purpose\n",
    "- If it takes too long to train, simplify your model, or use better feature preprocessing/selection\n",
    "\n",
    "The winner will present its notebook to the class during the reboot üí™\n",
    "\n",
    "(Don't forget, your Notebook should be made to be run from top to bottom in one go!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Some hints</summary>\n",
    "Take a closer look at feature engineering: Are there some features we haven't correctly preprocessed?  \n",
    "    \n",
    "Most of the time, a good dataset trumps a good model!\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install dependencies\n",
    "!pip install deap update_checker tqdm stopit joblib torch\n",
    "# install xgboost optionally\n",
    "!pip install xgboost\n",
    "# install tpot\n",
    "!pip install tpot\n",
    "import os\n",
    "from tpot import TPOTClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tpot = TPOTClassifier(generations=5, population_size=50, verbosity=2)\n",
    "# process autoML with TPOT\n",
    "tpot.fit(X_train, y_train)\n",
    "# print score\n",
    "print(tpot.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Optional) - Pipeline most steps (prepross & fit) in one single Sklearn Pipeline"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "230.797px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "368px",
    "left": "927px",
    "right": "20px",
    "top": "141px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
